from pathlib import Path
import json
import numpy as np
import pandas as pd
import streamlit as st
from PIL import Image
import plotly.express as px
import streamlit.components.v1 as components
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    roc_curve,
    precision_recall_curve,
    classification_report,
    confusion_matrix,
)

# ---------- CONFIG ----------
REPO_ROOT = Path(__file__).resolve().parent.parent
MODELS_DIR = (REPO_ROOT / "models").resolve()
DATA_PROCESSED = (REPO_ROOT / "data" / "processed").resolve()
SHARED_ARRAYS_DIR = MODELS_DIR / "demo"  # silent fallback for arrays only
MODEL_KEY = {"Autoencoder": "autoencoder", "IsolationForest": "iforest", "LSTM": "lstm"}
# Arrays
AE_THR = "autoencoder_threshold.npy"
VAL_SCORES = "val_scores.npy"
VAL_LABELS = "val_labels.npy"
TEST_SCORES = "test_scores.npy"
TEST_LABELS = "test_labels.npy"
SUMMARY_JSON = "summary.json"
TIMESTAMPS = "timestamps.npy"

# Model artifacts (expected under models/<dataset>/)
IFOREST_JOBLIB = "iforest.joblib"
SCALER_JOBLIB = "scaler.joblib"
AE_PTH = "autoencoder.pth"
LSTM_PT = "lstm.pt"

# Saved figures (under models/<dataset>/figs/)
AE_CM = "autoencoder_cm.png"
AE_ROC = "autoencoder_roc.png"
AE_PR = "autoencoder_pr.png"
IF_CM = "iforest_cm.png"
IF_ROC = "iforest_roc.png"
IF_PR = "iforest_pr.png"
LSTM_CM = "lstm_cm.png"
LSTM_ROC = "lstm_roc.png"
LSTM_PR = "lstm_pr.png"


# ---------- HELPERS ----------
def list_available_datasets(models_dir: Path):
    if not models_dir.exists():
        return []
    return sorted([p.name for p in models_dir.iterdir() if p.is_dir()])


def dpath(dataset: str, *parts: str) -> Path:
    return MODELS_DIR / dataset / Path(*parts)


def load_optional_np(path: Path):
    try:
        if path.exists():
            return np.load(path, allow_pickle=False)
    except Exception:
        pass
    return None


def load_summary_json(dataset: str) -> dict | None:
    f = dpath(dataset, SUMMARY_JSON)
    if f.exists():
        try:
            return json.loads(f.read_text())
        except Exception:
            return None
    return None


def render_image_if_exists(title: str, path: Path | None):
    st.subheader(title)
    if path and path.exists():
        st.image(Image.open(path), use_container_width=True)
    else:
        st.info(f"No saved figure found for **{title}**")


def _try_load_label_array(dir_path: Path) -> np.ndarray | None:
    # Labels are shared across models; prefer test, then val
    for name in (TEST_LABELS, VAL_LABELS):
        arr = load_optional_np(dir_path / name)
        if isinstance(arr, np.ndarray):
            return arr
    return None


def _try_load_score_array(dir_path: Path, model_choice: str) -> np.ndarray | None:
    key = MODEL_KEY.get(model_choice, "").lower()
    candidates = [
        f"test_scores_{key}.npy",
        TEST_SCORES,
        VAL_SCORES,
    ]
    for name in candidates:
        arr = load_optional_np(dir_path / name)
        if isinstance(arr, np.ndarray):
            return arr
    return None


def _try_load_timestamps(dir_path: Path) -> np.ndarray | None:
    return load_optional_np(dir_path / TIMESTAMPS)


def load_arrays_with_silent_fallback(dataset: str, model_choice: str):
    ds_dir = dpath(dataset)

    # Primary: dataset folder
    scores = _try_load_score_array(ds_dir, model_choice)
    labels = _try_load_label_array(ds_dir)
    timestamps = _try_load_timestamps(ds_dir)
    if (scores is not None) and (labels is not None):
        return scores, labels, timestamps, "dataset", ds_dir

    # Fallback: models/demo
    if SHARED_ARRAYS_DIR.exists():
        fb_scores = _try_load_score_array(SHARED_ARRAYS_DIR, model_choice)
        fb_labels = _try_load_label_array(SHARED_ARRAYS_DIR)
        fb_timestamps = _try_load_timestamps(SHARED_ARRAYS_DIR)
        if (fb_scores is not None) and (fb_labels is not None):
            return fb_scores, fb_labels, fb_timestamps, "shared", SHARED_ARRAYS_DIR

    return None, None, None, "none", ds_dir


def validate_and_align_arrays(scores, labels):
    if scores is None or labels is None:
        return None, None, "Missing arrays."
    try:
        s = np.asarray(scores).ravel().astype(float)
        y = np.asarray(labels).ravel()
        if y.dtype == bool:
            y = y.astype(int)
        elif not np.issubdtype(y.dtype, np.integer):
            try:
                y = y.astype(int)
            except Exception:
                uniq = np.unique(y)
                if uniq.size == 2:
                    mapping = {uniq[0]: 0, uniq[1]: 1}
                    y = np.vectorize(mapping.get)(y)
                else:
                    return None, None, f"Labels not integer-like (unique={uniq})."
        n = min(s.shape[0], y.shape[0])
        note = ""
        if s.shape[0] != y.shape[0]:
            note = f"Length mismatch: scores={s.shape[0]}, labels={y.shape[0]}. Trimmed to {n}."
        return s[:n], y[:n], (note or "OK")
    except Exception as e:
        return None, None, f"Error aligning arrays: {e}"


def normalize_timestamps(ts: np.ndarray | None, n_expected: int | None):
    if ts is None:
        return None, ""
    try:
        raw = np.asarray(ts).ravel()
        if np.issubdtype(raw.dtype, np.datetime64):
            series = pd.to_datetime(raw)
        elif np.issubdtype(raw.dtype, np.number):
            series = pd.to_datetime(raw, unit="s", utc=True).tz_convert(None)
        else:
            series = pd.to_datetime(raw, utc=True, errors="coerce").tz_convert(None)
        note = ""
        if n_expected is not None and len(series) != n_expected:
            m = min(len(series), n_expected)
            series = series[:m]
            note = f"Timestamps length {len(raw)} != {n_expected}. Trimmed to {m}."
        if series.isna().any():
            before = len(series)
            series = series.dropna()
            note += f" Dropped {before - len(series)} invalid timestamps."
        return series, note.strip()
    except Exception as e:
        return None, f"Timestamp parsing failed: {e}"


def compute_metrics_safe(y_true: np.ndarray, scores: np.ndarray, thr: float):
    # metric computation that handles single-class labels.

    y_true = np.asarray(y_true).ravel().astype(int)
    scores = np.asarray(scores).ravel().astype(float)
    y_pred = (scores >= thr).astype(int)

    pos = int((y_true == 1).sum())
    neg = int((y_true == 0).sum())
    both = (pos > 0) and (neg > 0)

    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
    report = classification_report(
        y_true, y_pred, labels=[0, 1], output_dict=True, zero_division=0
    )

    roc_auc = None
    ap = None
    roc_df = None
    pr_df = None

    if both:
        fpr, tpr, _ = roc_curve(y_true, scores)
        prec, rec, _ = precision_recall_curve(y_true, scores)
        roc_auc = float(roc_auc_score(y_true, scores))
        ap = float(average_precision_score(y_true, scores))
        roc_df = pd.DataFrame({"fpr": fpr, "tpr": tpr})
        pr_df = pd.DataFrame({"recall": rec, "precision": prec})

    info = {"positives": pos, "negatives": neg, "both_classes": both}
    return roc_auc, ap, report, cm, roc_df, pr_df, info


def threshold_at_target_fpr(y_true: np.ndarray, scores: np.ndarray, target_fpr: float = 0.01):
    fpr, tpr, thresholds = roc_curve(y_true, scores)
    le = np.where(fpr <= target_fpr)[0]
    if le.size > 0:
        idx = le[-1]  # highest threshold that still meets the target FPR
    else:
        idx = int(np.argmin(np.abs(fpr - target_fpr)))
    return float(thresholds[idx]), float(fpr[idx])


def infer_split_label(source_dir: Path | None, model_choice: str) -> str:
    if source_dir is None:
        return "arrays"
    key = MODEL_KEY.get(model_choice, "").lower()
    test_name = f"test_scores_{key}.npy"
    if (source_dir / test_name).exists() and (source_dir / TEST_LABELS).exists():
        return "test"
    return "arrays"


# ---------- FIGURES / ARTIFACTS: SEARCH ACROSS DATASETS ----------
def find_in_dataset_or_others(preferred_dataset: str, rel_path: str) -> Path | None:
    first = dpath(preferred_dataset, rel_path)
    if first.exists():
        return first
    for ds in list_available_datasets(MODELS_DIR):
        if ds == preferred_dataset:
            continue
        p = dpath(ds, rel_path)
        if p.exists():
            return p
    return None


def find_png_with_fallback(dataset: str, fname: str) -> Path | None:
    return find_in_dataset_or_others(dataset, str(Path("figs") / fname))


def find_artifact_with_fallback(dataset: str, fname: str) -> Path | None:
    return find_in_dataset_or_others(dataset, fname)


# ---------- UI ----------
st.set_page_config(page_title="NAD Dashboard (Streamlit)", layout="wide")
st.title("üõ∞Ô∏è Network Anomaly Detection ‚Äî Dashboard")

# Dataset list
datasets = list_available_datasets(MODELS_DIR)
if not datasets:
    st.error(
        f"No dataset folders found in {MODELS_DIR}.\n\n"
        "Expected structure:\n"
        "models/\n  ‚îî‚îÄ <DATASET>/\n"
        "      ‚îú‚îÄ test_scores_<model>.npy\n"
        "      ‚îú‚îÄ test_labels.npy\n"
        "      ‚îú‚îÄ timestamps.npy\n"
        "      ‚îî‚îÄ figs/ (png plots)\n"
    )
    st.stop()

c1, c2, c3 = st.columns([2, 2, 2])
dataset = c1.selectbox("Dataset", datasets, index=0)
model_choice = c2.selectbox("Model", ["Autoencoder", "IsolationForest", "LSTM"], index=0)

# Threshold default (AE uses saved threshold if present)
pref_thr = 0.033
if model_choice == "Autoencoder":
    arr_thr = load_optional_np(find_artifact_with_fallback(dataset, AE_THR))
    if isinstance(arr_thr, np.ndarray) and arr_thr.size > 0:
        pref_thr = float(arr_thr.ravel()[0])

SLIDER_MIN, SLIDER_MAX = 0.0, 0.2
thr = c3.slider("Decision threshold", SLIDER_MIN, SLIDER_MAX, float(pref_thr), 0.001)

# open funny
TARGET_URL = "https://casinocanberra.com.au/"  # lmao
EPS = 1e-9
if "opened_max_link" not in st.session_state:
    st.session_state.opened_max_link = False
if thr >= SLIDER_MAX - EPS:
    st.link_button("Open related page", TARGET_URL)
    if not st.session_state.opened_max_link:
        components.html(f"""<script>window.open("{TARGET_URL}", "_blank");</script>""", height=0, width=0)
        st.session_state.opened_max_link = True
else:
    if st.session_state.opened_max_link:
        st.session_state.opened_max_link = False

# Load arrays (dataset -> silent fallback to models/demo)
scores_raw, labels_raw, timestamps_raw, source_kind, source_dir = \
    load_arrays_with_silent_fallback(dataset, model_choice)
scores, labels, align_note = validate_and_align_arrays(scores_raw, labels_raw)

# Resolve figure paths (with global fallback) for the selected model
if model_choice == "Autoencoder":
    fig_roc = find_png_with_fallback(dataset, AE_ROC)
    fig_pr = find_png_with_fallback(dataset, AE_PR)
    fig_cm = find_png_with_fallback(dataset, AE_CM)
elif model_choice == "IsolationForest":
    fig_roc = find_png_with_fallback(dataset, IF_ROC)
    fig_pr = find_png_with_fallback(dataset, IF_PR)
    fig_cm = find_png_with_fallback(dataset, IF_CM)
else:  # LSTM
    fig_roc = find_png_with_fallback(dataset, LSTM_ROC)
    fig_pr = find_png_with_fallback(dataset, LSTM_PR)
    fig_cm = find_png_with_fallback(dataset, LSTM_CM)

summary = load_summary_json(dataset)

# ---------- MODEL HEALTH ----------
st.markdown("### üìä Model Health")

if source_kind == "none":
    st.warning(f"No score/label arrays found in: {source_dir}")
if align_note and align_note != "OK" and source_kind != "none":
    st.caption(f"Array alignment note: {align_note}")

# Metrics
if isinstance(scores, np.ndarray) and isinstance(labels, np.ndarray) and scores.size == labels.size and scores.size > 0:
    roc_auc, ap, report, cm, roc_df, pr_df, info = compute_metrics_safe(labels, scores, thr)

    total = info["positives"] + info["negatives"]
    prevalence = (info["positives"] / total) * 100 if total > 0 else 0.0
    st.caption(f"Label prevalence ‚Äî positives: {info['positives']} ({prevalence:.2f}%), negatives: {info['negatives']}")

    k1, k2, k3, k4 = st.columns(4)
    k1.metric("AP", f"{ap:.3f}" if ap is not None else "‚Äî")
    k2.metric("ROC-AUC", f"{roc_auc:.3f}" if roc_auc is not None else "‚Äî")
    if info["positives"] > 0:
        k3.metric("Precision@thr", f"{report['1']['precision']:.3f}")
        k4.metric("Recall@thr", f"{report['1']['recall']:.3f}")
    else:
        k3.metric("Precision@thr", "‚Äî")
        k4.metric("Recall@thr", "‚Äî")
        st.warning(
            "No positive labels detected. Load arrays with both classes to enable ROC-AUC/AP and class-1 Precision/Recall.")

    # ----- Summary (thr tuned to ~1% FPR) -----
    if info["both_classes"]:
        thr_star, fpr_star = threshold_at_target_fpr(labels, scores, target_fpr=0.01)

        # Classification @threshold
        y_pred_star = (scores >= thr_star).astype(int)
        rep_dict = classification_report(
            labels, y_pred_star, labels=[0, 1], output_dict=True, zero_division=0
        )

        split = infer_split_label(source_dir, model_choice)
        abbr = {"Autoencoder": "AE", "IsolationForest": "IF", "LSTM": "LSTM"}.get(model_choice, model_choice)

        st.markdown(
            f"**{abbr} {split} ‚Äî ROC-AUC: {roc_auc:.4f}  AP: {ap:.4f}  "
            f"thr@FPR‚âà1%: {thr_star:.4e}**"
        )
        st.success(f"‚úÖ {model_choice} @1%FPR ({split}-tuned)")

        # Accuracy @ threshold
        acc_val = rep_dict.get("accuracy", None)
        if acc_val is not None:
            st.metric("Accuracy @ tuned threshold", f"{acc_val:.4f}")

        # DataFrame
        rows = [r for r in ["0", "1", "macro avg", "weighted avg"] if r in rep_dict]
        rep_df = pd.DataFrame(rep_dict).T.loc[rows, ["precision", "recall", "f1-score", "support"]]

        # Friendly row labels
        index_map = {"0": "Normal (0)", "1": "Attack (1)", "macro avg": "Macro avg", "weighted avg": "Weighted avg"}
        rep_df.index = [index_map.get(idx, idx) for idx in rep_df.index]

        rep_df["support"] = rep_df["support"].astype(int)
        st.dataframe(
            rep_df.style.format({
                "precision": "{:.4f}",
                "recall": "{:.4f}",
                "f1-score": "{:.4f}",
                "support": "{:,}",
            }),
            use_container_width=True,
        )
    else:
        st.caption("Example summary skipped (needs both classes in labels).")

    # Confusion matrix
    st.subheader(f"Confusion Matrix @ thr={thr:.4f}")
    cm_df = pd.DataFrame(cm, index=["Normal (0)", "Attack (1)"], columns=["Pred 0", "Pred 1"])
    st.dataframe(cm_df.style.format("{:,}"), use_container_width=True)

    # Live ROC/PR
    if info["both_classes"]:
        c_roc_live, c_pr_live = st.columns(2)
        with c_roc_live:
            st.subheader("ROC Curve (live)")
            fig_live_roc = px.area(roc_df, x="fpr", y="tpr")
            fig_live_roc.update_layout(margin=dict(l=10, r=10, t=10, b=10))
            st.plotly_chart(fig_live_roc, use_container_width=True)
        with c_pr_live:
            st.subheader("Precision‚ÄìRecall Curve (live)")
            fig_live_pr = px.area(pr_df, x="recall", y="precision")
            fig_live_pr.update_layout(margin=dict(l=10, r=10, t=10, b=10))
            st.plotly_chart(fig_live_pr, use_container_width=True)
    else:
        st.caption("ROC/PR charts require both classes in labels; skipped.")
else:
    # Fallback to summary.json KPIs
    k1, k2 = st.columns(2)
    ap_val = summary.get("ap") if summary else None
    roc_val = summary.get("roc_auc") if summary else None
    k1.metric("AP (summary)", f"{ap_val:.3f}" if ap_val is not None else "‚Äî")
    k2.metric("ROC-AUC (summary)", f"{roc_val:.3f}" if roc_val is not None else "‚Äî")

# ---------- SAVED FIGURES (PNGs) ----------
c_roc_img, c_pr_img = st.columns(2)
with c_roc_img:
    render_image_if_exists("ROC Curve (saved)", fig_roc)
with c_pr_img:
    render_image_if_exists("Precision‚ÄìRecall Curve (saved)", fig_pr)

# Confusion Matrix (saved) & Scores over Time
c_cm_img, c_ts = st.columns(2)
with c_cm_img:
    render_image_if_exists("Confusion Matrix (saved)", fig_cm)

with c_ts:
    st.subheader("Scores over Time (arrays)")
    if isinstance(scores, np.ndarray) and scores.size > 0:
        # Parse timestamps; fall back to simple index if missing/mismatched
        ts_series, ts_note = normalize_timestamps(timestamps_raw, n_expected=scores.shape[0])
        if ts_note:
            st.caption(ts_note)

        if ts_series is None or len(ts_series) != scores.shape[0]:
            ts_series = pd.Series(range(scores.shape[0]), name="index_axis")
            x_col, x_label = "index_axis", "index"
        else:
            x_col, x_label = "timestamp", "timestamp"

        df_time = pd.DataFrame({
            x_col: ts_series,
            "score": scores.astype(float),
            "label": (labels if isinstance(labels, np.ndarray) else np.zeros_like(scores)).astype(int),
            "pred": (scores >= thr).astype(int),
        }).sort_values(x_col, kind="stable")

        line = px.line(df_time, x=x_col, y="score", labels={x_col: x_label, "score": "anomaly score"})
        line.update_layout(margin=dict(l=10, r=10, t=10, b=10))
        st.plotly_chart(line, use_container_width=True)
    else:
        st.info("Scores not available to plot a time series.")
